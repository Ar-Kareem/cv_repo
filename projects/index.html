<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> My Projects | <span class="font-weight-bold">Abdulrahman</span> Alabdulkareem </title> <meta name="author" content="Abdulrahman Alabdulkareem"> <meta name="description" content="Hi, I’m Abdulrahman Alabdulkareem. My friends call me Abdul. I’m an AI researcher focused on understanding LLM’s and Deep Learning models. I graduated from MIT with a dual M.S. in EECS and CSE as part of the InfoLab team and I currently work at Intelmatix. EXPERIENCE AI Scientist → Intelmatix, Riyadh, 2024-Current AI Research → InfoLab @ MIT, Cambridge, 2022-2024 Dual M.S. (GPA 5.0/5.0) → MIT, Cambridge, 2022-2024 Expl. Sys. Analyst → Aramco, Dhahran, 2020-2022 AI Internship → CCES, Cambridge/Riyadh, 2019-2019 AI Research → Honorio’s lab @ Purdue, West Lafayette, 2019-2020 Dual-major B.S. (GPA 4.0/4.0) → Purdue, West Lafayette, 2016-2020 "> <link rel="stylesheet" href="/cv_repo/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/cv_repo/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/cv_repo/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/cv_repo/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/cv_repo/assets/img/favicon.ico.png?9a8ca5177d334c7537c7149209a98e4c"> <link rel="stylesheet" href="/cv_repo/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ar-kareem.github.io/cv_repo/projects/"> <script src="/cv_repo/assets/js/theme.js?5bae44744eb1b157e4fb2a762ddb1c3b"></script> <link defer rel="stylesheet" href="/cv_repo/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/cv_repo//"> <span class="font-weight-bold">Abdulrahman</span> Alabdulkareem </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/cv_repo/">Home </a> </li> <li class="nav-item active"> <a class="nav-link" href="/cv_repo/projects/">My Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv_repo/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv_repo/assets/pdf/resume.pdf">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="https://www.linkedin.com/in/abdulrahman-alabdulkareem-493a8143/" rel="external nofollow noopener" target="_blank">LinkedIn </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <script type="text/javascript">
  document.addEventListener('DOMContentLoaded', function() {
    var observer = new IntersectionObserver(function(entries, obs) {
      entries.forEach(function(entry) {
        if (entry.isIntersecting) {
          entry.target.classList.add('in-view');
          obs.unobserve(entry.target);
        }
      });
    }, { threshold: 0.1 });
    document.querySelectorAll('.single-project').forEach(function(el) {
      observer.observe(el);
    });
  });
</script> <div class="my-post"> <header class="post-header"> <h1 class="post-title">My Projects</h1> <p class="post-description"></p> </header> <article> <div class="projects"> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title"></h2> <p class="my-description"></p> </div> </div> </div> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title">Poisson Equation Solver in Transformed Space</h2> <p class="my-description">Solving the Poisson equation is a critical task in almost all engineering fields, where it models countless phenomena like gravity and electrical potential. Many different Poisson equation solvers exist, but they rarely support solving the equation in a transformation on curved space. This publicly available project solves that problem by introducing an easy-to-use and lightweight Poisson equation solver implemented in Python that supports curved spaces, available on PyPI and GitHub. The solver supports Neumann, Dirichlet, and mixed boundary conditions, and is designed for ease of use, requiring less than 10 lines of code to set up and execute. </p> <div class="my-links-section"> <span class="label">Links:</span> <a class="link" href="https://pypi.org/project/poisson-transform/" target="_blank" rel="noopener noreferrer">PyPI</a> <a class="link" href="https://github.com/Ar-Kareem/poisson_transform/" target="_blank" rel="noopener noreferrer">GitHub</a> </div> </div> <div class="col-sm-6 mt-3 mt-md-0" style="display: flex; flex-direction: column; justify-content: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cv_repo/assets/img/POISSON-480.webp 480w,/cv_repo/assets/img/POISSON-800.webp 800w,/cv_repo/assets/img/POISSON-1400.webp 1400w," type="image/webp" sizes="250px"></source> <img src="/cv_repo/assets/img/POISSON.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0" style="display: flex; flex-direction: column; justify-content: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cv_repo/assets/img/MAP-480.webp 480w,/cv_repo/assets/img/MAP-800.webp 800w,/cv_repo/assets/img/MAP-1400.webp 1400w," type="image/webp" sizes="250px"></source> <img src="/cv_repo/assets/img/MAP.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title">Assessing Spatial Comprehension on Maps in Pre-trained AI Models</h2> <p class="my-description">Map reasoning is an intuitive skill for humans and a fundamental skill with important applications in many domains. In our blog post, we evaluate the capabilities of contemporary state-of-the-art Large Vision-Language Models (LVLMs) for reasoning on maps and comparing their capabilities with human participants on the coregistration task. We additionally propose and release a novel dataset to serve as an initial benchmark for map reasoning capabilities. We run an extensive analysis on the performance of open-source LVLMs showing that they struggle to achieve good performance on our dataset. Finally, we show that coregistration is intuitive to human participants who were able to reach close to perfect accuracy in a time-constrained manner. </p> <div class="my-links-section"> <span class="label">Links:</span> <a class="link" href="https://deep-learning-mit.github.io/staging/blog/2023/mapreason/" target="_blank" rel="noopener noreferrer">BlogPost</a> </div> </div> </div> </div> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title">Investigating the Dynamics of the Loss Hessian during training</h2> <p class="my-description">Recently in ML literature, the Hessian of the Loss in Deep Learning is a topic of great interest. In this work, we derive an efficient analytical computation of both the maximum eigenvalue and the trace of the Hessian of the Loss in addition to an analytical and empirical convergence rate of the algorithm. The algorithm is efficiently implemented using nothing but commonly used Machine Learning techniques: the forward pass and backpropagation. Then, we train over 150 models spanning over 10 thousand epochs with different hyperparameters and optimizers and run the algorithm after each epoch to empirically calculate the properties of the hessian during training. We provide empirical results for the relationship between generalizability and the hessian, in addition to several other observations that arise from this data. </p> <div class="my-links-section"> <span class="label">Links:</span> <a class="link" href="/cv_repo/assets/pdf/pdf_HESS.pdf" target="_blank" rel="noopener noreferrer">PDF</a> </div> </div> <div class="col-sm-6 mt-3 mt-md-0" style="display: flex; flex-direction: column; justify-content: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cv_repo/assets/img/HESS-480.webp 480w,/cv_repo/assets/img/HESS-800.webp 800w,/cv_repo/assets/img/HESS-1400.webp 1400w," type="image/webp" sizes="250px"></source> <img src="/cv_repo/assets/img/HESS.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0" style="display: flex; flex-direction: column; justify-content: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cv_repo/assets/img/NEWTON-480.webp 480w,/cv_repo/assets/img/NEWTON-800.webp 800w,/cv_repo/assets/img/NEWTON-1400.webp 1400w," type="image/webp" sizes="250px"></source> <img src="/cv_repo/assets/img/NEWTON.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title">Quasi-Newton methods for nonlinear systems of equations</h2> <p class="my-description">In this work, we give rise to the motivation of the root-finding problem and the use of Newton’s method within this context. Then, we discuss why one might go beyond those methods to introduce Quisi-Newton methods to which we rigorously derive both types of Broyden Updates along with an efficient implementation of the algorithm. We run my implementation of both types of Broyden Updates along with Newton’s method and an implementation of Broyden Updates provided by the Scipy library on two realistic real-world problems. We finally discuss the application of Broyden Updates in many different areas of science and the different variants of the algorithm that were later proposed. </p> <div class="my-links-section"> <span class="label">Links:</span> <a class="link" href="/cv_repo/assets/pdf/pdf_NEWTON.pdf" target="_blank" rel="noopener noreferrer">PDF</a> </div> </div> </div> </div> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title">Numerical Modelling of Aircraft Evacuations</h2> <p class="my-description">The FAA regulation for safe aircraft evacuations requires a demonstration under specific conditions. The current demonstrations are performed without any computational models aiding the design prior to live testing. This project demonstrates how a numerical model can be used to simulate a much wider range of conditions to aid designers in picking a single design to proceed with. Our numerical model is created as an equivalent circuit with non-linear constitutive equations. Employing the model on an example baseline configuration shows the location of bottlenecks and the relative impact of obstacles in the evacuation path. When paired together, a physical demonstration and numerical model can more effectively ensure that an aircraft is capable of a safe evacuation in all conditions, compared to a physical demonstration alone. </p> <div class="my-links-section"> <span class="label">Links:</span> <a class="link" href="/cv_repo/assets/pdf/pdf_EVACUATION.pdf" target="_blank" rel="noopener noreferrer">PDF</a> </div> </div> <div class="col-sm-6 mt-3 mt-md-0" style="display: flex; flex-direction: column; justify-content: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cv_repo/assets/img/EVACUATION2-480.webp 480w,/cv_repo/assets/img/EVACUATION2-800.webp 800w,/cv_repo/assets/img/EVACUATION2-1400.webp 1400w," type="image/webp" sizes="250px"></source> <img src="/cv_repo/assets/img/EVACUATION2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0" style="display: flex; flex-direction: column; justify-content: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cv_repo/assets/img/VQA2-480.webp 480w,/cv_repo/assets/img/VQA2-800.webp 800w,/cv_repo/assets/img/VQA2-1400.webp 1400w," type="image/webp" sizes="250px"></source> <img src="/cv_repo/assets/img/VQA2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title">Binary Visual Question Answering using Transformers with raw inputs</h2> <p class="my-description">In this work, we introduce the Visual-Question-Answering task and the balanced binary visual-question-answering dataset. We propose two models, one that is used as a baseline model which is a latent Joint-Embedding model that utilizes Transformer networks to embed the visual and textual parts of the question. We then propose our main model which is an attention model that also utilizes transformer networks as the backbone and is able to achieve relatively good results and beats our baseline latent Joint-Embedding model with the added benefit of being able to see the attention mask to visualize where the model is looking with respect to the question. Finally, we provide visualizations of our model applied to the test set which shows which parts of the image the model is looking at to answer the question. </p> <div class="my-links-section"> <span class="label">Links:</span> <a class="link" href="/cv_repo/assets/pdf/pdf_VQA.pdf" target="_blank" rel="noopener noreferrer">PDF</a> </div> </div> </div> </div> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title">Introduction to Markov Chain Monte Carlo (MCMC)</h2> <p class="my-description">This is a presentation I made and presented at an MIT class on Bayesian Learning. The presentation introduces the foundational concepts of Markov Chain Monte Carlo (MCMC) to students with prior knowledge of simpler statistical methods, such as rejection sampling and importance sampling. The session explores how MCMC improves parameter estimation in posterior distributions, addressing the limitations of basic techniques. After walking through the theoretical insights of MCMC and some practical demonstrations, the presentation ends with an application of MCMC in a Bayesian learning framework for a model built using Gen. The model detects linear regression parameters for a dataset while simultaneously assigning probabilities for outlier detection. </p> <div class="my-links-section"> <span class="label">Links:</span> <a class="link" href="/cv_repo/assets/pdf/pdf_MCMC.pptx" target="_blank" rel="noopener noreferrer">Slides</a> </div> </div> <div class="col-sm-6 mt-3 mt-md-0" style="display: flex; flex-direction: column; justify-content: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cv_repo/assets/img/MCMC-480.webp 480w,/cv_repo/assets/img/MCMC-800.webp 800w,/cv_repo/assets/img/MCMC-1400.webp 1400w," type="image/webp" sizes="250px"></source> <img src="/cv_repo/assets/img/MCMC.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0" style="display: flex; flex-direction: column; justify-content: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cv_repo/assets/img/BAYES-480.webp 480w,/cv_repo/assets/img/BAYES-800.webp 800w,/cv_repo/assets/img/BAYES-1400.webp 1400w," type="image/webp" sizes="250px"></source> <img src="/cv_repo/assets/img/BAYES.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title">Bayesian Recommender Systems</h2> <p class="my-description">With the proliferation of data nowadays, recommendation systems have become instrumental in filtering content for the user. These systems curate a set of personalized items to increase user satisfaction. In movie recommendation systems, the algorithm searches for content that would increase the user’s watch time. In this project, we design a Bayesian model to tackle the problem. Our goal is to predict the likelihood of a user liking an item. Experiments showed that our model is able to perform competitively with machine learning models. Moreover, in high confidence predictions, it surpasses them. However, the computational cost and lack of scalability of our model currently pose a limitation to its usage in a large-scale production setting. </p> <div class="my-links-section"> <span class="label">Links:</span> <a class="link" href="/cv_repo/assets/pdf/pdf_BAYES.pdf" target="_blank" rel="noopener noreferrer">PDF</a> </div> </div> </div> </div> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title">Comparative Analysis of Natural Language Methods for Part-of-Speech Tagging</h2> <p class="my-description">This project is a comparative analysis between large language models (LLMs) and traditional methods for part of speech tagging of natural language using a tagged subset of the widely used Penn Treebank dataset and a domain-specific dataset from the BioNLP STc challenge. LLMs, such as GPT-3 and BERT-style models, have shown remarkable performance in various NLP tasks, including part of speech tagging, and may offer advantages in terms of their ability to capture contextual information and handle long-range dependencies compared to more traditional methods. On the other hand, more traditional models have been widely used for part of speech tagging, exemplified by parsers such as the Stanford Parser, and the part of speech taggers available in the Natural Language Toolkit (NLTK) library in Python. We evaluate the accuracy of several models and provide insights into the strengths and weaknesses of each approach for part of speech tagging. This information informs the choice of modeling technique for similar applications and contributes to the understanding of the trade-offs between LLMs and PCFGs in part of speech tagging of natural language text. </p> <div class="my-links-section"> <span class="label">Links:</span> <a class="link" href="/cv_repo/assets/pdf/pdf_CFG.pdf" target="_blank" rel="noopener noreferrer">PDF</a> <a class="link" href="https://github.com/Ar-Kareem/Penn-Tree-Bank-Project" target="_blank" rel="noopener noreferrer">GitHub</a> </div> </div> <div class="col-sm-6 mt-3 mt-md-0" style="display: flex; flex-direction: column; justify-content: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cv_repo/assets/img/CFG-480.webp 480w,/cv_repo/assets/img/CFG-800.webp 800w,/cv_repo/assets/img/CFG-1400.webp 1400w," type="image/webp" sizes="250px"></source> <img src="/cv_repo/assets/img/CFG.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0" style="display: flex; flex-direction: column; justify-content: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cv_repo/assets/img/SYMBOLIC-480.webp 480w,/cv_repo/assets/img/SYMBOLIC-800.webp 800w,/cv_repo/assets/img/SYMBOLIC-1400.webp 1400w," type="image/webp" sizes="250px"></source> <img src="/cv_repo/assets/img/SYMBOLIC.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title">Identifying Symbolic Communication in a Simulated Teacher-Student Environment</h2> <p class="my-description">Symbolic communication is an inherent and intuitive aspect of the human experience. In this project, we propose, implement, and run inference on a probabilistic Bayesian model for identifying symbolic communication. We focused on a recently proposed simulated teacher-student environment where we have access to human data. We show several qualitative and quantitative results that compare our model with human judgments. These results suggest that our approach is reasonably effective at identifying symbolic communication with adequate accuracy. We utilize the Gen probabilistic programming framework for the implementation of our model. </p> <div class="my-links-section"> <span class="label">Links:</span> <a class="link" href="/cv_repo/assets/pdf/paper_symbolic.pdf" target="_blank" rel="noopener noreferrer">PDF</a> </div> </div> </div> </div> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title">Learning to Navigate in Human Crowds</h2> <p class="my-description">Social robot navigation requires that the robot follows social norms while navigating towards its goal. Current algorithms model pedestrians as independent agents, making the problem computationally intractable and degrading overall performance in dense human crowds. In this work, we explore various ways to enhance the robot’s performance in environments with a higher number of pedestrians and aim to achieve well-behaved scaling. Specifically, we compare different approaches common in the literature such as state reduction, reward shaping, and curriculum learning. We find that the use of curriculum learning closely approximates optimal (human-like) behavior. The report serves as supplemental information, while the presentation includes the results of the experiments including animated visualizations. </p> <div class="my-links-section"> <span class="label">Links:</span> <a class="link" href="/cv_repo/assets/pdf/pdf_PATHFIND.pptx" target="_blank" rel="noopener noreferrer">Slides</a> <a class="link" href="/cv_repo/assets/pdf/pdf_PATHFIND.pdf" target="_blank" rel="noopener noreferrer">PDF</a> </div> </div> <div class="col-sm-6 mt-3 mt-md-0" style="display: flex; flex-direction: column; justify-content: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cv_repo/assets/img/PATHFIND-480.webp 480w,/cv_repo/assets/img/PATHFIND-800.webp 800w,/cv_repo/assets/img/PATHFIND-1400.webp 1400w," type="image/webp" sizes="250px"></source> <img src="/cv_repo/assets/img/PATHFIND.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="single-project"> <div class="row justify-content-sm-center text-left" style="margin-top:0.75cm; margin-bottom:0.75cm;"> <div class="col-sm-6 mt-3 mt-md-0" style="display: flex; flex-direction: column; justify-content: center;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cv_repo/assets/img/YOONPRESENT-480.webp 480w,/cv_repo/assets/img/YOONPRESENT-800.webp 800w,/cv_repo/assets/img/YOONPRESENT-1400.webp 1400w," type="image/webp" sizes="250px"></source> <img src="/cv_repo/assets/img/YOONPRESENT.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <h2 class="my-project_title">Paper Recap: Mapping Language Models To Grounded Conceptual Spaces </h2> <p class="my-description">This is a presentation I made and presented at an MIT class on large language models by Professor Yoon Kim. The presentation is a recap of the paper "Mapping Language Models to Grounded Conceptual Spaces" which addresses a critical limitation of text-only language models: their lack of grounding, or the ability to connect linguistic representations with real-world referents. Despite this challenge, the paper demonstrates that these models exhibit a robust conceptual understanding, enabling inference and fluent text generation. Our presentation summarizes the key findings of the paper and then introduces original experiments that explore this alignment further. Our results reveal both supporting evidence that confirms the paper’s findings and contrasting outcomes that highlight areas of disagreement. </p> <div class="my-links-section"> <span class="label">Links:</span> <a class="link" href="/cv_repo/assets/pdf/pdf_YOONPRESENT.pdf" target="_blank" rel="noopener noreferrer">Slides</a> </div> </div> </div> </div> </div> </article> </div> </div> <div class="more-section"> <h2>Interested in More?</h2> <p> View my <a href="/cv_repo//assets/pdf/resume.pdf" class="more-link">Resume</a> or <a href="mailto:arkareem@mit.edu?subject=Website%20Contact" class="more-link">Contact Me</a> </p> </div> <footer class="sticky-bottom" role="contentinfo"> <div class="container"> © Copyright 2025 Abdulrahman Alabdulkareem. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/cv_repo/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/cv_repo/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/cv_repo/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/cv_repo/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/cv_repo/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/cv_repo/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/cv_repo/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/cv_repo/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> </body> </html>